{"meta":{"title":"Welcome to my world","subtitle":"I am Kevin","description":"This is my blog.I will write down what I know","author":"Kevin","url":"https://kevinascd.github.io","root":"/"},"pages":[{"title":"about","date":"2021-04-26T10:04:35.000Z","updated":"2021-04-26T10:04:35.452Z","comments":false,"path":"about/index.html","permalink":"https://kevinascd.github.io/about/index.html","excerpt":"","text":""}],"posts":[{"title":"卷积神经网络模型的可解释性","slug":"Untitled","date":"2021-08-14T09:50:43.000Z","updated":"2021-08-14T09:51:39.088Z","comments":true,"path":"2021/08/14/Untitled/","link":"","permalink":"https://kevinascd.github.io/2021/08/14/Untitled/","excerpt":"","text":"缺乏可解释性仍然是在许多应用中采用深层模型的一个关键障碍。在这项工作中，明确地调整了深层模型，这样人类用户可以在很短的时间内完成他们预测背后的过程。具体地说，训练了深度时间序列模型，使得类概率预测具有较高的精度，同时被节点较少的决策树紧密地建模。使用直观的玩具例子以及治疗白血症和HIV的医疗任务，这种新的树正则化产生的模型比简单的L1或L2惩罚更容易模拟，而不牺牲预测能力。 使用普通的反向传播得到的图像噪声较多，基本看不出模型的学到了什么东西。使用反卷积可以大概看清楚猫和狗的轮廓，但是有大量噪声在物体以外的位置上。导向反向传播基本上没有噪声，特征很明显的集中猫和狗的身体部位上。 虽然借助反卷积和导向反向传播“看到”了CNN模型神秘的内部，但是却并不能拿来解释分类的结果，因为它们对类别并不敏感，直接把所有能提取的特征都展示出来了。在刚才的图片中，模型给出的分类结果是猫，但是通过反卷积和导向反向传播展示出来的结果却同时包括了狗的轮廓。换句话说，并不知道模型到底是通过哪块区域判断出当前图片是一只猫的。要解决这个问题，必须考虑其他办法。 对时间序列模型的树正则化方法进行评估。将重点放在GRU-RNN模型上，以后将对新的混合GRU-HMM模型进行一些实验。与MLP一样，每个正则化技术（tree，L2，L1）都可以应用于GRU的输出节点上的一系列强度参数λ。重要的是，算法可以计算给定参数的任何固定深度模型的平均决策树路径长度，因此可以用于测量任何正则化（包括L1或L2）下的决策边界复杂性。这意味着，在训练任何模型时，都可以跟踪预测性能（按ROC曲线（AUC）下的面积测量；较高的值意味着更好的预测），以及解释每个模型所需的决策树的复杂性（按的平均路径长度度量；较低的值意味着更多的可解释模型）。展示没有任何相关深度模型的基线独立决策树分类的结果，扫描控制分支大小的一系列参数，以探索该基线如何权衡路径长度和预测质量。 CAM 大家在电视上应该都看过热成像仪生成的图像 图像中动物或人因为散发出热量，所以能够清楚的被看到。接下来要介绍的CAM(Class Activation Mapping)产生的CAM图与之类似，当需要模型解释其分类的原因时，它以热力图（Saliency Map，不知道怎么翻译最适合，叫热力图比较直观一点）的形式展示它的决策依据，如同在黑夜中告诉哪有发热的物体。 对一个深层的卷积神经网络而言，通过多次卷积和池化以后，它的最后一层卷积层包含了最丰富的空间和语义信息，再往下就是全连接层和softmax层了，其中所包含的信息都是人类难以理解的，很难以可视化的方式展示出来。所以说，要让卷积神经网络的对其分类结果给出一个合理解释，必须要充分利用好最后一个卷积层。 CAM利用GAP(Global Average Pooling)替换掉了全连接层。可以把GAP视为一个特殊的average pool层，只不过其pool size和整个特征图一样大，其实说白了就是求每张特征图所有像素的均值. GAP的优点在NIN的论文中说的很明确了：由于没有了全连接层，输入就不用固定大小了，因此可支持任意大小的输入；此外，引入GAP更充分的利用了空间信息，且没有了全连接层的各种参数，鲁棒性强，也不容易产生过拟合； 还有很重要的一点是，在最后的 mlpconv层(也就是最后一层卷积层)强制生成了和目标类别数量一致的特征图，经过GAP以后再通过softmax层得到结果，这样做就给每个特征图赋予了很明确的意义，也就是categories confidence maps。 如果当时不理解这个categories confidence maps是个什么东西，结合CAM应该就能很快理解。 重点看下经过GAP之后与输出层的连接关系(暂不考虑softmax层)，实质上也是就是个全连接层，只不过没有了偏置项，如图所示： 对每一个类别C，每个特征图k的均值都有一个对应的w，记为_wck_wkc 。 CAM的基本结构就是这样了，下面就是和普通的CNN模型一样训练就可以了。训练完成后才是重头戏：如何得到一个用于解释分类结果的热力图呢？其实非常简单，比如说要解释为什么分类的结果是羊驼，把羊驼这个类别对应的所有_wck_wkc 取出来，求出它们与自己对应的特征图的加权和即可。由于这个结果的大小和特征图是一致的，需要对它进行上采样，叠加到原图上去，如下所示。 CAM以热力图的形式告诉了，模型是重点通过哪些像素确定这个图片是羊驼了。 Grad-CAM****方法 前面看到CAM的解释效果已经很不错了，但是它有一个致使伤，就是它要求修改原模型的结构，导致需要重新训练该模型，这大大限制了它的使用场景。如果模型已经上线了，或着训练的成本非常高，几乎是不可能为了它重新训练的。于是乎，Grad-CAM横空出世，解决了这个问题。 Grad-CAM的基本思路和CAM是一致的，也是通过得到每对特征图对应的权重，最后求一个加权和。整体结构如下图所示 注意这里和CAM的另一个区别是，Grad-CAM对最终的加权和加了一个ReLU，加这么一层ReLU的原因在于只关心对类别c有正影响的那些像素点，如果不加ReLU层，最终可能会带入一些属于其它类别的像素，从而影响解释的效果。使用Grad-CAM对分类结果进行解释的效果如下图所示： 除了直接生成热力图对分类结果进行解释，Grad-CAM还可以与其他经典的模型解释方法如导向反向传播相结合，得到更细致的解释。 这样就很好的解决了反卷积和导向反向传播对类别不敏感的问题。当然，Grad-CAM的神奇之处还不仅仅局限在对图片分类的解释上，任何与图像相关的深度学习任务，只要用到了CNN，就可以用Grad-CAM进行解释，如图像描述(Image Captioning)，视觉问答(Visual Question Answering)等，所需要做的只不过是把yc换为对应模型中的那个值即可。 限于篇幅，本文就不展开了，更多细节，强烈建议大家去读读论文，包括Grad-CAM与CAM权重等价的证明也在论文中。如果只是想在自己的模型中使用Grad-CAM，可以参考这个链接，熟悉tensorflow的话实现起来真的非常简单，一看就明白。 LIME 前面共同的局限性：当模型对来说完全为一个黑盒时就无能为力了。针对这个问题，这里介绍另一套办法，即使对模型一无所知也能够对它的行为作出解释。 LIME是KDD 2016上一篇非常漂亮的论文，思路简洁明了，适用性广，理论上可以解释任何分类器给出的结果。其核心思想是：对一个复杂的分类模型(黑盒)，在局部拟合出一个简单的可解释模型，例如线性模型、决策树等等。这样说比较笼统，从论文中的一张示例图来解释： 如图所示，红色和蓝色区域表示一个复杂的分类模型（黑盒），图中加粗的红色十字表示需要解释的样本，很难从全局用一个可解释的模型（例如线性模型）去逼近拟合它。 但是，当把关注点从全局放到局部时，可以看到在某些局部是可以用线性模型去拟合的。具体来说，从加粗的红色十字样本周围采样，所谓采样就是对原始样本的特征做一些扰动，将采样出的样本用分类模型分类并得到结果（红十字和蓝色点），同时根据采样样本与加粗红十字的距离赋予权重（权重以标志的大小表示）。虚线表示通过这些采样样本学到的局部可解释模型，在这个例子中就是一个简单的线性分类器。在此基础上，就可以依据这个局部的可解释模型对这个分类结果进行解释了。 一个看似复杂的模型通过巧妙的转换，就能够从局部上得到一个让人类理解的解释模型，光这样说还是显得有些空洞，具体来看看LIME在图像识别上的应用。希望LIME最好能生成和Grad-CAM一样的热力图解释。但是由于LIME不介入模型的内部，需要不断的扰动样本特征，这里所谓的样本特征就是指图片中一个一个的像素了。存在一个问题，LIME采样的特征空间太大的话，效率会非常低，而一张普通图片的像素少说也有上万个。若直接把每个像素视为一个特征，采样的空间过于庞大，严重影响效率；如果少采样一些，最终效果又会比较差。 所以针对图像任务使用LIME时还需要一些特别的技巧，也就是考虑图像的空间相关和连续的特性。不考虑一些极小特例的情况下，图片中的物体一般都是由一个或几个连续的像素块构成，所谓像素块是指具有相似纹理、颜色、亮度等特征的相邻像素构成的有一定视觉意义的不规则像素块，称之为超像素。相应的，将图片分割成一个个超像素的算法称为超像素分割算法，比较典型的有SLIC超像素分割算法还有quickshit等，这些算法在scikit-image库中都已经实现好了，quickshit分割后如图所示： 从特征的角度考虑，实际上就不再以单个像素为特征，而是以超像素为特征，整个图片的特征空间就小了很多，采样的过程也变的简单了许多。更具体的说，图像上的采样过程就是随机保留一部分超像素，隐藏另一部分超像素，如下所示： 从图中可以很直观的看出这么做的意义：找出对分类结果影响最大的几个超像素，也就是说模型仅通过这几个像素块就已经能够自信的做出预测。这里还涉及到一个特征选择的问题，毕竟不可能穷举特征空间所有可能的样本，所以需要在有限个样本中找出那些关键的超像素块。虽然这部分没有在论文中过多提及，但在LIME的代码实现中是一个重要部分，实现了前向搜索（forward selection）、Lasso和岭回归（ridge regression）等特征选择方式，默认当特征数小于等于6时采用前向搜索，其他情况采用岭回归。 整体流程如图: 和Grad-CAM一样，LIME同样可以对其他可能的分类结果进行解释。 LIME除了能够对图像的分类结果进行解释外，还可以应用到自然语言处理的相关任务中，如主题分类、词性标注等。因为LIME本身的出发点就是模型无关的，具有广泛的适用性。 虽然LIME方法虽然有着很强的通用性，效果也挺好，但是在速度上却远远不如Grad-CAM那些方法来的快。当然这也是可以理解的，毕竟LIME在采样完成后，每张采样出来的图片都要通过原模型预测一次结果。 比较GRUs与各种正则化的实验的主要结论概述如下。 树正则化模型比其他形式的正则化有更少的节点。通过任务，发现在小决策树（低平均路径长度）的目标区域中，提出的树正则化实现了更高的预测质量（更高的AUCs）。在信号和噪声HMM任务中，当树的平均路径长度为10时，树正则化（图3（d）中的绿线）实现了接近0.9的AUC值。具有L1或L2正则化的类似模型仅使用复杂度几乎为2倍的树（路径长度超过25）达到该AUC。在任务（图4）中，看到在路径长度为2-10时AUC增加0.05-0.1。在TIMIT任务（图5a）中，看到路径长度为20-30时的AUC增益为0.05-0.1。在图5b中的HIV CD4血细胞计数任务中，看到路径长度为10-15的AUC在0.03和0.15之间的差异。图5d中的HIV粘附任务在19到25的路径长度范围内的AUC增益在0.03到0.05之间，而在较小的路径上，所有方法都非常差，这表明问题的难度。总之，这些AUC增益在确定如何实施后续的HIV治疗中特别有用。对于独立决策树（橙色线）、L1正则化深度模型（红色线）或L2正则化深度模型（蓝色线），树正则化通常在短路径长度上实现高auc的最佳点。在非自举实验中，还测试了L1和L2惩罚的线性组合弹性网正则化（Zou和haste2005）。发现弹性网遵循与L1和L2相同的趋势线，没有明显的差异。在需要人的可模拟性的领域中，在小复杂度的情况下，预测精度的提高可能意味着提供任务价值的模型与不可用的模型之间的差异，这可能是因为性能太差或预测不可预测。 结论 介绍了一种新的层次树正则化技术，它鼓励任何可微模型的复杂决策边界都能很好地用人工模拟函数来逼近，从而使领域专家能够快速理解和近似计算更复杂的模型在做什么。总的来说，培训程序是稳健和有效的；未来的工作可以继续探索和提高学习模型的稳定性，并确定如何将方法应用于输入本身不可解释的情况（例如图像中的像素）。在三个复杂的现实世界领域（HIV治疗、脓毒症治疗和人类语音处理）中，树正则化模型在更简单、近似人类模拟模型的情况下提供了预测精度的提高。未来的工作可以将树正则化应用于局部的、具体的损失近似（Ribeiro、Singh和Guestrin 2016）或表示学习任务（鼓励简单边界的嵌入）。更广泛地说，一般训练程序可以将树正则化或其他程序正则化应用于广泛的一类流行模型，帮助超越稀疏性，走向人类可以轻松模拟并因此信任的模型。","categories":[],"tags":[],"author":"Kevin"},{"title":"2021.6.1 恶意软件 FireBall 安全报告","slug":"wsefdc","date":"2021-07-26T06:56:58.000Z","updated":"2021-07-26T06:57:45.814Z","comments":true,"path":"2021/07/26/wsefdc/","link":"","permalink":"https://kevinascd.github.io/2021/07/26/wsefdc/","excerpt":"","text":"一、 综述6月1日，知名安全公司CheckPoint发布报告称，发现了由中国公司控制的流氓软件“火球（Fireball）”，因受害者众多，已经引起国外安全机构的重视。 在“火球（Fireball）”事件中，火绒安全团队发现了野马浏览器、Deal Wifi软件等8款流氓软件，这些流氓软件感染电脑后会将Chrome浏览器的首页、TAB页改为随机生成的搜索页，而用户无法更改。虽然页面各不相同，但搜索页均抓取雅虎和谷歌数据，火绒安全团队推测，流氓软件制造者以控制用户点击雅虎和谷歌的广告牟利。 流氓软件在安装时会检测电脑是否有Chrome浏览器，若没有则相安无事，若有则会提示用户安装一个Chrome插件，不安装插件就不能安装软件。 虽然这些软件来自国内卿烨科技、百盛达科技等多家公司，但是火绒安全团队通过追踪发现，其均由同一作者“&#x62;&#x61;&#x6f;&#121;&#117;&#52;&#51;&#x30;&#64;&#103;&#109;&#97;&#105;&#x6c;&#46;&#x63;&#x6f;&#x6d;”制作。作者注册不同网站，制作了一批流氓软件。 这些软件只攻击Chrome浏览器，但考虑到Chrome浏览器在国外的市场占有率，“火球（Fireball）”事件可谓影响巨大，国内Chrome用户也可能收到挟持。 用户可以通过卸载这些流氓软件恢复Chrome浏览器的设置。当然，另一种更省力的方法是开启火绒安全软件，火绒安全软件已可全面查杀“火球（Fireball）”事件涉及的流氓软件，建议用户下载安装最新版火绒安全软件。 这次“火球（Fireball）”事件虽然在外国爆发，但其“作案手法”在国内早已屡见不鲜，可以看出国内的网络犯罪手法正在向国际蔓延。 二、 事件分析近期火球（FireBall）事件中，涉事软件存在劫持Chrome浏览器首页及新标签页的恶意行为。经过火绒追查，发现更多软件涉及此次事件，如下图所示： 软件列表 以“Deal WiFi”软件为例，安装如下图所示，如果用户不勾选 “Set http://mystart.dealwifi.com as your chrome homepage and new tab”，则无法继续安装。如下图所示： 安装 勾选后使用火绒剑监控“Deal WiFi”安装过程，可以看到程序在后台安装了一个Chrome插件，如下图所示： 安装Chrome插件 该插件会“劫持”Chrome的设置界面，如下图所示： 劫持Chrome首页及新标签创建页面 Chrome浏览器的首页被修改为hxxps://mystart.dealwifi.com/?type=apps，如下图所示： 搜索劫持 这些流氓程序安装流程一样，都会强制安装一个名称和所装软件名称一样的Chrome插件。这些插件功能完全相同，都是锁定首页和新标签页的URL，其中名为”Soso Desktop”的流氓软件还强制修改默认搜索引擎。 与国内一般的添加带有首页推广号的锁首方式不同，病毒插件锁定的根据安装的流氓软件不同搜索页面也不相同如下表： 不同软件劫持的网址 我们通过对比搜索结果可以发现，除 Holainput锁定的搜索页面最终结果会跳转Google外，其余搜索页面和hxxps://http://www.yahoo.com的搜索结果一致，后台疑似使用Yahoo的搜索结果。但是无论使用的是Google还是Yahoo，病毒服务器都可以记录用户的搜索内容，对用户的搜索信息隐私安全造成威胁。 经过火绒追查，诸多上述恶意软件的注册信息中都出现了注册人鲍雨与其注册所使用电子邮箱“&#98;&#97;&#111;&#x79;&#x75;&#52;&#51;&#48;&#x40;&#x67;&#x6d;&#97;&#x69;&#108;&#x2e;&#x63;&#111;&#109;”。通过搜索卿烨科技有限公司的工商信息，我们发现名为卿烨科技的公司共有五家，其中与病毒存在直接关系的公司共有三家，分别为卿烨科技（北京）有限责任公司（下文称北京卿烨）、卿烨科技（上海）有限责任公司北京卿烨雨林分公司（下文称上海卿烨北京分公司）和卿烨科技（上海）有限责任公司（上海卿烨）。经过我们对企业信息的梳理与筛查，我们初步理清了与病毒相关的公司运作关系，如下图所示： 涉事公司关系图 在整个公司运营中，最主要的涉事人为马琳和鲍雨，马琳为相关公司的最主要出资人，鲍雨为主要经理人。 北京朗基努斯投资中心股东信息中，只有马琳和鲍雨，该公司以相对控股方式控制卿烨科技（上海）有限责任公司。该公司的主要职能为进行资本，进行对外投资整合资源。 上海卿烨主要负责开发进行流量劫持的浏览器插件和国内外相关网站服务的开发，并且通过对外投资以绝对控股方式控制着北京卿烨，同时设有下属分支公司上海卿烨北京分公司。在该公司产权信息中，我们发现了传播恶意插件的软件，如下图所示： 上海卿烨产权信息 我们还在招聘网站找到了该公司的招聘信息，如下图所示： 上海卿烨招聘信息 北京卿烨主要负责软件及游戏开发，其开发的软件为劫持流量的传播载体，软件诸如：Deal WiFi、Soso Desktop和FVP Imageviewer等。在该公司产权信息中，我们发现了更多携带流氓推广的软件，如下图所示： 北京卿烨产权信息 上海卿烨北京分公司主要负责流量劫持的浏览器开发。该公司公示的招聘信息，如下图所示： 上海卿烨北京分公司招聘信息 三、 附录样本SHA1： 0f6df3d425c0f2e60eca1cd8a20106c305296f23 08731ac376f3d6af690d45214d4644f3c42930a1 2a8d9d7210f216f00aa9c7d301d7ceb95aa37fad 64c92cc638e3be9377d03209eda8b785ceb5de4e ff22a77a03c10e2ad244923f4e94d64e00551a94 b7f4442990811a1c456bce83f403febefdac6cc6 9b78982db7520f226169cd1bb6a704a7dfac951c 23e2b70c2070e15e993bd00f3be8afb89111b92b 117d558fca1c7dcfff59702cb9393486ec368e47 ae76db7f24a111ca022b00d29fb08cc76cbab41b","categories":[],"tags":[],"author":"Kevin"},{"title":"面试官如何判断面试者的机器学习水平？","slug":"面试官如何判断面试者的机器学习水平？","date":"2021-07-05T11:26:31.000Z","updated":"2021-07-05T11:33:09.313Z","comments":true,"path":"2021/07/05/面试官如何判断面试者的机器学习水平？/","link":"","permalink":"https://kevinascd.github.io/2021/07/05/%E9%9D%A2%E8%AF%95%E5%AE%98%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E9%9D%A2%E8%AF%95%E8%80%85%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B0%B4%E5%B9%B3%EF%BC%9F/","excerpt":"","text":"那对于算法工程师什么是实力的体现呢？我认为主要是1. 理论基本功 2. 工程能力 3. 业务理解能力。这也是面试官判断面试者机器学习水平的三个重要维度。 理论基本功 - 机器学习与深度学习的算法知识 数学原理和机器学习基础很重要，面试三板斧LR，SVM，GBDT的原理是必须要搞清楚的，内部细节也要懂，最好自己实现过才讲得清。例如你说你用过xgboost，那你就要说得清xgboost好在哪里，有哪些改进。特征工程也很重要，是基本功。现在算法更新很快，推陈出新，只有基本功好，才能快速跟上前沿。强烈推荐李航的统计学习方法，真心很精练了。 此外，针对只有炼丹（深度学习）经验的同学，我还喜欢问的问题： RNN、LSTM、GRU时序模型的特点 梯度弥散与梯度爆炸的原因与解决方案 激活函数的优缺点 什么是L2正则化，为什么L2正则化可以防止过拟合？L1正则化与L2正则化有什么区别和联系。 2. 工程能力 初学者的一大误区就是觉得深度学习就是网上找个公开数据集，github上找个开源代码，然后开始花里胡哨就完了。实际工作中算法工程师对工程能力的要求非常是非常硬核的。首先算法工程师是一名工程师。模型训练出来绝对不是万事大吉了，只是模型落地的第一步。 模型太重速度太慢，不符合线上需求，怎么办？ 模型部署和推理，如何提速？怎么做模型的压缩、剪枝、量化quantization？ 现在我们训练数据轻轻松松上千万视频，数据的预处理也成为了主要瓶颈（bottleneck）之一，如何给数据预处理提速？ 前面说的都是深度学习的，然而很多场景是不适合用深度学习的。很多线上模型用的都是XGBoost。如何给XGBoost加速呢？ 当然，这些都是针对深度学习领域工程能力的考查，此外也会考查传统的算法题与数据结构（LeetCode），有些情况也会问一些system design的问题。 3. 业务能力 考查业务能力的主要是业务场景题，让面试者设想一个业务场景，考查他该如何解决？ 对于预测蔬菜价格的场景如何构建特征，主要是从哪几个纬度思考？为什么要构建某个特征。 给定某用户的app里浏览信息，如何判断此人是否有车？ 对于xxx任务如何构造一个稳定、可靠、精准的评测集？ 现在，算法工程师的要求的确水涨船高，基本要求候选人三个方面都没有短板。理论基本功，只要好好学过斯坦福的那几门课（cs229, cs231, cs224）问题到不大。业务场景题，有过实际经验的同学，也基本能说上两句。最要命的恰恰是工程能力，很多同学都是在工程能力上折戟成沙。 还好英特尔工程师们分析人工智能领域遇到的种种计算瓶颈，把他们的解决方案用更贴近硬件的方式整合进英特尔至强处理器里，为人工智能算法提供澎湃动力，也能解决算法工程师们的燃眉之急。回到我前面提到的工程能力的几个难题： 模型部署和推理，如何提速？ 英特尔至强可扩展处理器集成 AVX-512 指令集，为算法模型开辟了专用快车道。英特尔开发的oneDNN函数库在AVX-512 指令集进一步开发， 它不仅集成有众多优化的深度学习基元，例如直接批量卷积、池化、激活等，可提升各类深度学习应用的效率，更可针对 CPU 的各项特性实施优化。轻松将算法模型中的各种操作向量化为 AVX-512 指令，并在支持 AVX-512 指令集的在英特尔至强可扩展处理器中带来强劲算力。 英特尔oneDNN 中的深度学习基元向量化为 AVX-512 指令 怎么做模型的压缩、剪枝、量化呢？ 大多数 AI 模型多采用传统的 FP32 数据格式，其实这种格式完全可以在损失很小精度的前提下，转换成 BF16 或 INT8 格式，以换取更高的处理效率或者说吞吐量。但人为转换费时费力，不仅无法根据处理器平台特性实施优化，且转换后的模型也无法兼容不同的硬件平台。 为此，英特尔推出了 OpenVINO工具套件，则是专业和省心工具的代表，它提供的模型量化功能，为上述问题提供了应对良方。它能让基于不同 AI 框架，如 TensorFlow、MXNet、PyTorch 等构建的 FP32 数据格式 AI 模型，在损失很少精度的情况下转化为 INT8 和 BF16 数据格式。同时可以利用英特尔DL Boost技术把对低精度数据格式的操作指令融入到了 AVX-512 指令集中，即 AVX-512_VNNI (矢量神经网络指令) 和 AVX-512_BF16（bfloat16），对模型的训练和推理都提供了强大的支持。 使用 OpenVINO量化压缩的模型服务，其推理性能是普通的 Tensorflow Serving推理性能的3.4倍，效果群拔。 CDS 首云针对不良视频内容检测场景的验证测试结果（https://www.intel.cn/content/www/cn/zh/now/data-centric/openvino-tool-suite-ai-workshop.html） 数据的预处理也成为了主要瓶颈（bottleneck）之一，如何给数据预处理提速？ 英特尔的OpenVINO工具套件，对传统的 OpenCV 图像处理库也进行了指令集优化，实现了性能与速度的显著提升。 如下图所示，在典型的图像抠图应用场景中，OpenVINO工具套件带来了 5 倍的推理速度提升，而在不良内容检测和文本检测场景中，效率提升更是达到了 6 倍和 11 倍之多。 OpenVINO™ 工具套件在爱奇艺 AI 应用场景中带来的性能提升（https://www.intel.cn/content/www/cn/zh/now/data-centric/openvino-tool-suite-ai-workshop.html） 如何给XGBoost加速呢？ 这时候就可以用上英特尔开发的数据分析加速库DAAL（Data Analytics Acceleration Library）。在英特尔至强处理器上，英特尔优化了XGBoost 代码库，最新的优化成果已经集成到 XGBoost 1.0 及之后的版本。相比 XGBoost 0.9 版，新版本性能提升 2 倍以上，最高达 54 倍。 基于英特尔DAAL优化的xgboost（英特尔中国制造行业AI实战手册） 总而言之，算法同学非常值得深入学习英特尔一系列工具箱，如OpenVINO、oneDNN 、DAAL等，尤其是它们所属的 oneAPI 统一编程模型。这是一个跨架构工具组合，帮助开发者简化异构编程的流程、加速性能、提升生产力。凭借这些高级工具，开发者可在英特尔CPU、GPU、FPGA上实现AI工作负载的加速，并使代码可以在目前及未来的英特尔处理器及加速器上运行。 掌握好oneAPI，便可以快速拥有算法模型上线部署的各种能力，可以轻松地将训练好模型在英特尔平台的各种处理芯片上快速部署起来，提高自己的算法落地能力！","categories":[],"tags":[],"author":"Kevin"},{"title":"最简单的用django搭建网站教程——从连python都没安装到用手机访问自己编写的网页","slug":"最简单的用django搭建网站教程——从连python都没安装到用手机访问自己编写的网页","date":"2021-07-05T11:25:16.000Z","updated":"2021-07-05T11:25:20.198Z","comments":true,"path":"2021/07/05/最简单的用django搭建网站教程——从连python都没安装到用手机访问自己编写的网页/","link":"","permalink":"https://kevinascd.github.io/2021/07/05/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%94%A8django%E6%90%AD%E5%BB%BA%E7%BD%91%E7%AB%99%E6%95%99%E7%A8%8B%E2%80%94%E2%80%94%E4%BB%8E%E8%BF%9Epython%E9%83%BD%E6%B2%A1%E5%AE%89%E8%A3%85%E5%88%B0%E7%94%A8%E6%89%8B%E6%9C%BA%E8%AE%BF%E9%97%AE%E8%87%AA%E5%B7%B1%E7%BC%96%E5%86%99%E7%9A%84%E7%BD%91%E9%A1%B5/","excerpt":"","text":"前言当下，最好的展示程序结果的方式，一定是html网页了。不管是简易性还是通用性， 用html网页可以让你轻松地把自己的程序通过网址分享给大家来浏览，而无需他们安装软件或是打开文件，甚至在手机上都可以轻易访问。 Python下最著名的框架就是Django了， 作为曾经学过JSP并苦java久矣的笔者， 想通过这篇博客， 用最简单的教程记录和分享这最简单的网站框架——从没有python到手机访问自己的网页。 准备工作（环境已搭好的可跳过） 1. 下载python和pycharmpython下载链接首先下载python环境， 选择最新的python版本下载即可。pycharm下载链接然后下载pycharm，这是最好的python编辑器， 也必须下载。 可以下载免费的社区版，也可以自行破解/购买专业版。打开pycharm， 随便新建一个工程如test， 进入界面。 2. 下载django框架在pycharm页面按快捷键Alt+F12, 弹出terminal终端窗口。 在其中输入pip install -i https://pypi.douban.com/simple/ django即可完成对django的下载。 这句代码用了douban的镜像，加速一下下载速度。 因为我早就下载过了， 所以这里显示的是already satisfied。 用django搭建网站继续上面的， 在terminal内 输入django-admin startproject mysite然后django就会自动帮你创建一个名为”mysite”的网站文件夹，五脏俱全。 此时你的工程结构应该和我一样： 这时候网站已经自动搭建成功了，我们开始运行： 继续在terminal内输入： python mysite/manage.py runserver,结果应该如图所示： 不用去管这个具体什么意思， 点击上面的链接， 即http://127.0.0.1:8000， 没有问题的话，应该就可以得到下图的效果啦： 这是django默认的效果，显然，后面我们可以对这个网页进行各种自定义，做出各种花样的自己想要的效果。 不过这里，我希望的是可以用最简单的教程示例把整个搭建网站的过程写下来， 涉及网页设计的这里就跳过了， 就以这个网页为例继续示范如何用手机访问搭好的网站。 这里强烈图鉴django教程，有兴趣自己编写网站的读者可以去学习django教程 手机与外网访问网页一个网站写出来只能自己本地通过127.0.0.1看， 无疑是乏味的， 那么如何能够轻易地分享给大家，让大家都能访问呢？ 用django其实非常容易实现！ 内网访问首先，先确保内网（同一wifi下）的机器可以访问。 这里需要做两步至关重要的操作： 修改访问的权限 django默认是他人不能访问的， 我们现在希望能让他人访问网页，因此，找到mysite下，setting.py文件修改设置： 第28行这里，在中括号内加入 *， 即改为：ALLOWED_HOSTS = [&#39;*&#39;]代表所有用户都能访问这个网页。 如果不做这一步的话会报错。 通过0.0.0.0来运行 在terminal中按ctrl + c来终止刚刚运行的网站， 重新输入以下命令运行： python mysite/manage.py runserver 0.0.0.0:8000 这里指定了通过0.0.0.0的8000端口运行，这样非本地也能访问了。 （之前是默认127.0.0.1:8000本地访问） 做完这两步后，就可以在内网内用手机查看电脑写的网页啦。首先用win+R，输入cmd，进入命令行工具 接着输入ipconfig命令， 查询本机的内网IP： 以我为例， IPv4地址就是10.222.185.59. 记一下。 这时， 你在本机电脑上直接打开http://0.0.0.0:8000，是无法打开的： 但是，你可以用连接在同一wifi下的手机或者电脑（即保证在同一局域网内）， 用浏览器， 打开http://0.0.0.0:8000，是不是发现，可以得到刚刚一样的网页？ 也就是说， 已经可以在内网内成功访问本机所搭建的网站了！ 外网映射显然，位于同一wifi下这个限制太严格， 我们希望可以分享给任何人。 那么就需要进行外网映射，即达到通过一个域名网址就能访问的效果。 就像我们输入http://www.baidu.com就能访问百度一样。 百度外网映射，有很多软件可以实现， 我这里以国内最知名的花生壳软件举例：首先下载花生壳：https://hsk.oray.com/download/打开软件后， 注册账户并登陆： 看到如上界面，点击右下角加号： 进行内网映射的编辑： 应用名称随便写， 应用类型选择HTTP， 然后外网域名这一行默认生成的，不用管。 内网主机这里，填刚刚通过ipconfig得到的IP地址，即内网分配的地址。 内网端口，设为我们运行的端口，默认即8000.保存。 在刚刚的花生壳界面可以看到： 用浏览器输入该域名，是不是发现访问到了我们搭建的django网站？同样的，用手机4G，在safari等浏览器中输入该域名， 也可以看到网站。 至此，我们实现了从0到外网也可以访问搭建的网站的过程。 结语路漫漫其修远兮，django是一个非常大的框架， 除此之外，想做好网页还需要html之类的前端知识。 这篇博客只是用最简单的步骤，写下了从0到发布网站的过程。 千里之行始于足下，后面可以对django进行深度地学习，编写自己的网页。","categories":[],"tags":[],"author":"Kevin"},{"title":"构建深度神经网络，我有20条「不成熟」的小建议 ","slug":"构建深度神经网络，我有20条「不成熟」的小建议-机器之心","date":"2021-07-05T11:24:08.000Z","updated":"2021-07-26T06:58:28.175Z","comments":true,"path":"2021/07/05/构建深度神经网络，我有20条「不成熟」的小建议-机器之心/","link":"","permalink":"https://kevinascd.github.io/2021/07/05/%E6%9E%84%E5%BB%BA%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8C%E6%88%91%E6%9C%8920%E6%9D%A1%E3%80%8C%E4%B8%8D%E6%88%90%E7%86%9F%E3%80%8D%E7%9A%84%E5%B0%8F%E5%BB%BA%E8%AE%AE-%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83/","excerpt":"","text":"在我们的机器学习实验室中，我们已经在许多高性能的机器上进行了成千上万个小时的训练，积累了丰富的经验。在这个过程中，并不只有电脑学习到了很多的知识，事实上我们研究人员也犯了很多错误，并且修复了很多漏洞。 在本文中，我们将根据自身经验（主要基于 TensorFlow）向大家提供一些训练深度神经网络的实用秘诀。有些建议可能对你来说可能已经很熟悉了，但是其他人可能并不太了解。另外还有些建议可能并不适用，甚至可能对于特定的任务来说是不好的建议，所以请谨慎使用！ 这些都是一些广为人知的方法，我们也是站在了巨人的肩膀上！本文的目的只是高屋建瓴地对如何在实践中使用它们进行总结。 通用秘诀 使用 ADAM 优化器。它确实很有效，相对于较传统的优化器（如原版梯度下降），我们更喜欢使用 ADAM。在 TensorFlow 环境下使用 ADAM 时，请注意：如果你想要保存和恢复模型权重，请记住在设置完 AdamOptimizer 后设置 Saver，这是因为 ADAM 也有需要恢复的状态（即对应于每个权重的学习率）。 ReLU 是最好的非线性（激活函数），这就好比 Sublime 是最好的文本编辑器。但说实话，ReLU 确实是运行速度最快、最简便的，而且令人惊讶的是，它们在工作时梯度并不会逐渐减小（从而能够防止梯度消失）。尽管 sigmoid 是一个常用激活函数，但是它在 DNN 中传播梯度的效果并不太好。 不要在输出层使用激活函数。这应该是显而易见的，但是如果你通过一个共用的函数构建每一层，那这可能是一个很容易犯的错误：请确保在输出层不要使用激活函数。 为每一层添加一个偏置项。这是机器学习的入门知识：本质上，偏置项将一个平面转换到最佳拟合位置。在 y=mx+b 式中，b 是偏置项，使直线能够向上或向下移动到最佳的拟合位置。 使用方差缩放初始化。在 TensorFlow 中，该方法写作 tf.contrib.layers.variance_scaling_initializer()。根据我们的实验，这种初始化方法比常规高斯分布初始化、截断高斯分布初始化及 Xavier 初始化的泛化/缩放性能更好。粗略地说，方差缩放初始化根据每一层输入或输出的数量（在 TensorFlow 中默认为输入的数量）来调整初始随机权重的方差，从而帮助信号在不需要其他技巧（如梯度裁剪或批归一化）的情况下在网络中更深入地传播。Xavier 和方差缩放初始化类似，只不过 Xavier 中每一层的方差几乎是相同的；但是如果网络的各层之间规模差别很大（常见于卷积神经网络），则这些网络可能并不能很好地处理每一层中相同的方差。 白化（归一化）输入数据。在训练中，令样本点的值减去数据集的均值，然后除以它的标准差。当网络的权重在各个方向上延伸和扩展的程度越小，你的网络就能更快、更容易地学习。保持数据输入以均值为中心且方差不变有助于实现这一点。你还必须对每个测试输入也执行相同的归一化过程，所以请确保你的训练集与真实数据类似。 以合理地保留动态范围的方式对输入数据进行缩放。这个步骤和归一化有关，但是应该在归一化操作之前进行。例如，在真实世界中范围为 [0, 140000000] 的数据 x 通常可以用「tanh(x)」或「tanh(x/C)」来进行操作，其中 C 是某个常数，它可以对曲线进行拉伸，从而在 tanh 函数的动态倾斜（斜率较大）部分对更大输入范围内的数据进行拟合。尤其是在输入数据在函数的一端或者两端都不受限的时候，神经网络将在数据处于 (0,1) 时学习效果更好。 一般不要使用学习率衰减。在随机梯度下降（SGD）中，降低学习率是很常见的，但是 ADAM 天然地就考虑到了这个问题。如果你真的希望达到模型性能的极致，请在训练结束前的一小段时间内降低学习率；你可能会看到一个突然出现的很小的误差下降，之后它会再次趋于平缓。 如果你的卷积层有 64 或 128 个滤波器，这就已经足够了。特别是对于深度网络来说，比如 128 个滤波器就已经很多了。如果你已经拥有了大量的滤波器，那么再添加更多的滤波器可能并不会提升性能。 池化是为了变换不变性（transform invariance）。池化本质上是让网络学习到图像「某个部分」的「一般概念」。例如，最大池化能够帮助卷积网络对图像中特征的平移、旋转和缩放具备一定的鲁棒性。 神经网络的调试 如果网络学习效果很差（指网络在训练中的损失/准确率不收敛，或者你得不到想要的结果），你可以试试下面的这些秘诀： 过拟合！如果你的网络学习效果不佳，你首先应该做的就是去过拟合一个训练数据点。准确率基本上应该达到 100% 或 99.99%，或者说误差接近 0。如果你的神经网络不能对一个数据点达到过拟合，那么模型架构就可能存在很严重的问题，但这种问题可能是十分细微的。如果你可以过拟合一个数据点，但是在更大的集合上训练时仍然不能收敛，请尝试下面的几条建议。 降低学习率。你的网络会学习地更慢，但是它可能会找到一个之前使用较大的步长时没找到的最小值。（直观地说，你可以想象一下你正在走过路边的沟渠，此时你想要走进沟的最深处，在那里模型的误差是最小的。） 提高学习率。这将加快训练速度，有助于加强反馈回路（feedback loop）。这意味着你很快就能大概知道你的网络是否有效。尽管这样一来网络应该能更快地收敛，但是训练结果可能不会太好，而且这种「收敛」状态可能实际上是反复震荡的。（使用 ADAM 优化器时，我们认为在许多实验场景下，~0.001 是比较好的学习率。） 减小（小）批量处理的规模。将批处理大小减小到 1 可以向你提供与权重更新相关的更细粒度的反馈，你应该将该过程在 TensorBoard（或者其他的调试/可视化工具）中展示出来。 删掉批归一化层。在将批处理大小减小为 1 时，这样做会暴露是否有梯度消失和梯度爆炸等问题。我们曾经遇到过一个好几个星期都没有收敛的网络，当我们删除了批归一化层（BN 层）之后，我们才意识到第二次迭代的输出都是 NaN。在这里使用批量归一化层，相当于在需要止血带的伤口上贴上了创可贴。批归一化有它能够发挥效果的地方，但前提是你确定自己的网络没有 bug。 加大（小）批量处理的规模。使用一个更大的批处理规模——还觉得不够的话，如果可以，你不妨使用整个训练集——能减小梯度更新的方差，使每次迭代变得更加准确。换句话说，权重更新能够朝着正确的方向发展。但是！它的有效性存在上限，而且还有一些物理内存的限制。我们发现，这条建议通常不如前两个建议（将批处理规模减小到 1、删除批归一化层）有用。 检查你矩阵的重构「reshape」。大幅度的矩阵重构（比如改变图像的 X、Y 维度）会破坏空间局部性，使网络更不容易学习，因为这时网络也必须学习重构。（自然特征变得支离破碎。事实上自然特征呈现出空间局部性也是卷积神经网络能够如此有效的原因！）使用多个图像/通道进行重构时要特别小心；可以使用 numpy.stack() 进行适当的对齐操作。 仔细检查你的损失函数。如果我们使用的是一个复杂的函数，可以试着把它简化为 L1 或 L2 这样的形式。我们发现 L1 对异常值不那么敏感，当我们遇到带有噪声的批或训练点时，可以进行稍小幅度的调整。 如果可以，仔细检查你的可视化结果。你的可视化库（matplotlib、OpenCV 等）是否调整数据值的范围或是对它们进行裁剪？你可以考虑使用一种视觉上均匀的配色方案。 案例研究 为了使上文描述的过程更有关联性，下面给出了一些用于描述我们构建的卷积神经网络的部分真实回归实验的损失图（通过 TensorBoard 进行可视化）。 最初，网络完全没有学习： 我们试着裁剪数据值，防止它们超越取值范围： 看看这些没有经过平滑的值有多么「疯狂」！学习率太高了吗？我们试着降低学习率，并且在一组输入数据上进行训练： 你可以看到学习率最初的几个变化发生在哪里（大约训练了 300 步和 3000 步时）。显然，这里我们进行的学习率下降调整太快了。所以如果给它更长的学习率衰减时间，它将表现得更好（损失更低）： 可以看到，学习率在第 2000 步和第 5000 步时下降。这种情况更好，但是仍然不够完美，因为损失并没有降到 0。 然后我们停止学习率衰减，并且尝试通过 tanh 函数将输入值移动到一个更狭窄的范围内。这很显然将误差值带到了 1 以下，但是我们始终不能过拟合训练集： 在这里我们发现了，通过删除批归一化层，网络很快地在一两次迭代之后输出 NaN。我们禁用了批归一化，并将初始化方法改为方差缩放法。这让一切都不一样了！我们可以过拟合仅仅包含一两个输入的测试集。然而，下面的图对 Y 轴进行了裁剪。初始误差值远远高于 5，这说明误差减小了近 4 个数量级： 上方的图是非常平滑的，但是你可以看到，它极其迅速地过拟合了测试输入，并且随着时间推移，整个训练集的损失降到了 0.01 以下。这个过程没有降低学习率。之后，我们在学习率降低了一个数量级之后继续训练，得到了更好的结果： 这些结果要好得多！但是如果我们以几何级别降低学习率，而不是将训练分成两部分，会如何呢？ 在每一步中将学习率乘以 0.9995，结果不是很好： 这大概是因为学习率下降地太快了。乘数如果取 0.999995 会更好，但是结果和完全不衰减相差无几。我们从这个特定的实验序列中得出结论：批归一化隐藏了糟糕的初始化导致的梯度爆炸；并且除了在最后故意设计的一个学习率衰减可能有帮助，减小学习率对 ADAM 优化器并没有特别的帮助。与批归一化一样，对值进行裁剪掩盖了真正的问题。我们还通过 tanh 函数控制高方差的输入值。 我们希望这些基本的诀窍在你对构建深度神经网络更加熟悉的时候能够提供帮助。通常，正是简单的事情让一切变得不同。","categories":[],"tags":[],"author":"Kevin"},{"title":"社会工程简要说明（","slug":"社会工程简要说明","date":"2021-06-18T10:02:53.000Z","updated":"2021-06-18T10:11:03.313Z","comments":true,"path":"2021/06/18/社会工程简要说明/","link":"","permalink":"https://kevinascd.github.io/2021/06/18/%E7%A4%BE%E4%BC%9A%E5%B7%A5%E7%A8%8B%E7%AE%80%E8%A6%81%E8%AF%B4%E6%98%8E/","excerpt":"","text":"行为心理学篇1、人比较瘦的话，这个人一定很精明，你占不到他一点光。比较胖，则这个人心胸宽阔，不计较琐事。 2、人比较矮的话，一定要高看它一眼，他一定会有所长处，值得你去学习。人比较高的话，此人一定可能存在高傲自满的状态。 3、让一个人来握手的时候一定要注意他的举动，是左手还是右手相握，还有握的力度，时间等等，来观察此人是否尊重别人。 4、“口头语”特别重要，一个人的随口说出来的话就可以看出一个人的素质所在。 5、和人在一起，要学会多聆听，这样才能从说话看出人心。 看衣着打扮-读性格气质1、一个人戴帽子，可以看出他对头部建立的形象。 2、一个人化妆，说明她在细心雕刻自己的性格。 3、从一个人的穿鞋，可以看出他对生活的品味。 4、从服装可以看出一个人的品味。 5、从香水，看透女人的内心，不同的味道不同的心理。 爱“屋”及“乌”-兴趣爱好识人心1、吃东西，可以影响一个人的心情，有时候心情很坏，吃到特别喜欢的会瞬间变好。 2、读书，说明这个人很有品味。这个互联网电子书爆棚的时代，如果还看书，真的很有品味。 3、旅游，喜欢旅游的人心态都特别好。 4、做家务，喜欢做家务的人，可以反映出另一个自己。 5、运动，喜欢的运动可以看出一个人的个性，比如打篮球，打乒乓球。 （1）头部动作的心理解读 a.低下头可以表示缺乏信心。如果是在别人赞扬时低下了头，他们可能是由于害羞或胆怯。 b.触摸或揪着耳朵表明其内心犹豫不决 c.一个真诚的微笑整个脸部肌肉都会动作，而虚伪的微笑通常只会使嘴唇起作用。 d.下意识的倾头可以表明对某事或某人的兴趣，但过度倾斜的头部可能是同情的标志。 e.当一个人在思考或思考时，通常会把手放在脸颊上。 f.当听众点头时，这通常是一个积极的信息，并表明他们感兴趣并注意。但是，过度点头可能意味着听者失去了兴趣，他们在伪装 g.向别人伸出下巴可能表示蔑视。 （2）上半身动作心理解读 a.向后推肩膀可以显示出力量和勇气（对他人和自己都如此）。 b.张开双臂意味着人们乐于接近，愿意交谈。而无意识双臂交叉则通常表示不满。 c.双手放在脖子后面，触摸颈部可以显示某人对正在发生的事情感兴趣 e.手掌向上和向外表示积极和开放的心理状态。 f.如果一个人的手指交错或手指尖压在一起，通常他们在思考。 （3）下半身动作心理解读 a.把手放在你的臀部可以显示出渴望和准备（注意，有时后这也是侵略心理的信号）。 b.双脚分开代表着支配和掌控的心理，同时当一个人坐着双腿张开时，则表明他们此时很有安全感 c.双腿交叉意味着几件事：放松，舒适；紧张；厌烦或失去耐心——这取决于腿部肌肉的紧张程度。 d.脚尖无意间指的方向代表你更感兴趣的方向。 e.坐下时，如果一个脚踝位于另一条腿的膝盖上时，代表着强烈的自信。 （4）眼睛的心理解读 a.眼神的下降可以表达恐惧，内疚，害羞或顺从。 b.压低眉毛，眯起的眼睛表示某人正试图理解所说的内容 c.如果感到紧张或试图去评价别人，人们往往更容易眨眼。 d.如果你直视别人的眼睛，你就会表现出自信。 e.如果你在谈话中被别人激怒了，一个常见的下意识的动作就是快速扫视对方。 通过揣摩去分析心理，例如：（b.张开双臂意味着人们乐于接近，愿意交谈。而无意识双臂交叉则通常表示不满。）看见某公司前台正在张开双臂，就可以假装把自己水杯打翻，弄湿简历，并让前台帮忙重新打印一份（他肯定会打印的），把携带木马的U盘给他，插入电脑，就成公入侵了前台电脑，也就成功入侵了网络。 揣摩篇（鬼谷子原文） 1、必以其甚喜之时，往而极其欲也，其有欲也，不能隐其情。☆释义：当年要游说他人时，应在对方最高兴的时候去游说，然后继续使其愿望极度膨胀。只要对方有欲望，他就不愿隐瞒事物的真实情况。俗话说，人逢喜事精神爽，说话也要看看对方心情好坏，且在对方心情好时，游说他人的效果更好。 估计曾国藩也领悟透彻了鬼谷子的这句话，于是曾国藩说：“劝人不可指其过，须先美其长。人喜则语言易入，怒则语言难入，怒胜私故也”，意思就是说：要劝谏他人过错时，先要表扬他人的优点长处，他人一高兴，你再说话就会效果更好，事半功倍！所以，我们学习鬼谷子揣摩人心的第一种方法，那就是在他人高兴时候和他交流，并说好听话让他更加膨胀，然后我们就可以看透他的真心，继而驾驭人心了 2、必以其甚惧之时，往而极其恶也，具有恶也，不能隐其情：情欲必知其变。☆释义：在对方怀疑、戒惧时去做游说，一定要在对方对所厌恶的事情恨至极点时候再去。倘若对方有所厌恶的事，此时和他交流，他也不会隐瞒其真实情况。因为对方在有欲求的时候，常会反映在他们外在神态变化之中。这句话的意思和“上帝欲将你灭亡，先让你疯狂”有些相似，而这里所说的疯狂是让对方愤怒到疯狂！ 明知道他人非常生气某事，再在他人最生气的时候去拜访，然后继续火上浇油，别人肯定会毫无保留地将他心中怒火和不满，以及所有心思都倾诉给你，你掌握了他人的所思所想，也就可以驾驭人心了。鬼谷子的以上两种揣摩术都是利用了人性的弱点——情绪失控（大喜大怒），然后展开攻势，最后达到自己目的！ 3、感动而不知其变者，乃且错其人勿与语，而更问所亲，知其所安。☆释义：如果对方有所感动，却不显露在外部神态中，可先不要着急，不要与他直面地讨论，可与他说一些另外他能熟悉的东西，说一些使他感到亲近的事情，就可以知道他安身立命不露神色的依据。这种方法就是用在那些喜怒不形于色的人身上，和他们谈某事，不要单刀直入，要学会婉转迂回，先打感情牌，用谈心来拉近彼此距离。 鬼谷子知道，是人都有弱点，只是某些人比较善于伪装情绪罢了，所以我们要做的就是：说些他熟悉的事物，一步步打开他们的心扉！“是狐狸总会露出尾巴”，而那些心中有欲求的人，总会在不经意间表现在外部神态。这个时候我们就要仔细察言观色，最后探知他人心灵深处的欲求，然后根据他人的欲求去驾驭他们！ 注：要站在对方的角度考虑问题，解决事情往往简单的多。","categories":[],"tags":[],"author":"Kevin"},{"title":"数据挖掘概要","slug":"数据挖掘","date":"2021-06-13T07:10:00.000Z","updated":"2021-06-13T07:22:36.547Z","comments":true,"path":"2021/06/13/数据挖掘/","link":"","permalink":"https://kevinascd.github.io/2021/06/13/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/","excerpt":"","text":"前言数据挖掘是通过对大量数据的清理及处理以发现信息，并应用于分类，推荐系统，预测等方面的过程。 一、数据挖掘过程1.数据选择 分析业务需求后，选择应用于需求业务相关的数据：业务原始数据、公开的数据集、也可通过爬虫采集网站结构化的数据。明确业务需求并选择好针对性的数据是数据挖掘的先决条件。 2.数据预处理 通常选择好的数据会有噪音，不完整等缺陷，需要对数据进行清洗，缺失项处理，集成，转换以及归纳： python字符串处理（相当方便）、正则式匹配、pandas、beautifulsoup处理Html标签等等工具。 3.特征工程/数据转换 根据选择的算法，对预处理好的数据提取特征，并转换为特定数据挖掘算法的分析模型。 4.数据挖掘 使用选择好的数据挖掘算法对数据进行处理后得到信息。 5.解释与评价 对数据挖掘后的信息加以分析解释，并应用于实际的工作领域。 二、数据挖掘常用算法简介1.关联分析算法关联规则在于找出具有最小支持度阈值和最小置信度阈值的不同域的数据之间的关联。在关联规则的分析算法研究中，算法的效率是核心的问题。 经典的算法有：Apriori算法，AprioriTid算法，FP-growth算法； 2.分类算法决策树算法：以树形结构表示分类或者决策集合，产生规则或者发现规律。主要有ID3算法，C4.5算法， SLIQ算法， SPRINT算法， RainForest算法； 朴素Bayes分类算法：利用Bayes定理概率统计的方法，选择其中概率比较大的类别进行分类； CBA(Classification Based on Association)算法：基于关联规则的分类算法； MIND(Mining in Database)算法 ：采用数据库中用户定义的函数(user-definedfunction，简称UDF)来实现分类的算法； 神经网络分类算法：利用训练集对多个神经的网络进行训练，并用训练好的模型对样本进行分类； 粗集理论：粗集理论的特点是不需要预先给定某些特征或属性的数量描述，而是直接从给定问题出发，通过不可分辨关系和不可分辨类确定问题的近似域,从而找出问题中的内在规律； 遗传算法：遗传算法是模拟生物进化过程，利用复制(选择)、交叉(重组)和变异(突变)3个基本方法优化求解的技术； 3.聚类算法聚类分析与分类不同，聚类分析处理的数据对象的类是未知的。聚类分析就是将对象集合分组为由类似的对象组成 的多个簇的过程。分为3类方法： Ipartitioning method(划分方法) 给定1个N个对象或者元组的数据库，1个划分方法构建数据的K个划分，每1个划分表示1个聚簇，并且K&lt;N。经典算法是K-MEAN(K平均值)； hierarchical method(层次方法) 对给定数据对象集合进行层次的分解，经典算法是BIRTH算法； grid based method(基于网格的方法) 这种方法采用一个多分辨率的网格数据结构。将空间量化为有限数目的单元，这些单元形成了网格结构，所有聚类分析都在网格上进行。常用的算法有STING，SkWAVECLUSTER和 CLIQUE； 小结随着数据量的日益积累以及数据库种类的多样化，各种数据挖掘方法作用范围有限，都有局限性，因此采用单一方法难以得到决策所需的各种知识。但它们的有机组合具有互补性，多方法融合将成为数据挖掘算法的发展趋势。 三、数据挖掘算法实现1、相关知识(1)距离度量：在数据挖掘中需要明确样本数据相似度，通常可以计算样本间的距离，如下为常用距离度量的介绍。样本数据以： 曼哈顿距离： 也称曼哈顿街区距离，就如从街区的一个十字路口点到另一个十字路口点的距离， 二维空间（多维空间按同理扩展）用公式表示为 欧氏距离：表示为点到点的距离。二维空间（多维空间按同理扩展）的公式表示为 闵可夫斯基距离：是一组距离方法的概括，当 p=1 既是曼哈顿距离，当 p=2 既是欧氏距离。当p越大，单一维度的差值对整体的影响就越大。 闵可夫斯基距离（包括欧氏距离，曼哈顿距离）的优缺点： 优点：应用广泛。 缺点：无法考虑各分量的单位以及各分量分布（方差，期望）的差异性。（其中个分量的单位差异可以使用数据的标准化来消除，下面会有介绍。） 余弦相关系数：样本数据视为向量，通过两向量间的夹角余弦值确认相关性，数值范围[-1，1]。 -1表示负相关，0表示无关，1表示正相关。 余弦相关系数的优缺点： 优点：余弦相似度与向量的幅值无关，只与向量的方向相关，在文档相似度（TF-IDF）和图片相似性（histogram）计算上都有它的身影； 而且在样本数值稀疏的时候仍可以使用。 缺点：余弦相似度受到向量的平移影响，上式如果将 x 平移到 x+1, 余弦值就会改变。(可以理解为受样本的起始标准的影响，接下来介绍的皮尔逊相关系数可以消除这个影响) 皮尔逊相关系数：计算出了样本向量间的相关性，数值范围[-1，1]。 考虑计算的遍历的次数，有一个替代公式可以近似计算皮尔逊相关系数： 皮尔逊相关系数优点：可消除每个分量标准不同（分数膨胀）的影响，具有平移不变性和尺度不变性。 (2)数据标准化：参考文章各分量计算距离而各分量的单位尺度差异很大，可以使用数据标准化消除不同分量间单位尺度的影响，，加速模型收敛的效率，常用的方法有三种： min-max 标准化：将数值范围缩放到（0,1）,但没有改变数据分布。max为样本最大值，min为样本最小值。 z-score 标准化：将数值范围缩放到0附近, 经过处理的数据符合标准正态分布。u是平均值，σ是标准差。 修正的标准z-score：修正后可以减少样本数据异常值的影响。将z-score标准化公式中的均值改为中位数，将标准差改为绝对偏差。 其中asd绝对偏差：u为中位数，card(x)为样本个数 (3) 算法的效果评估：十折交叉验证：将数据集随机分割成十个等份，每次用9份数据做训练集，1份数据做测试集，如此迭代10次。十折交叉验证的关键在于较平均地分为10份。 N折交叉验证又称为留一法：用几乎所有的数据进行训练，然后留一个数据进行测试，并迭代每一数据测试。留一法的优点是：确定性。 2、协同过滤推荐算法代码实现、数据集及参考论文 电影推荐——基于用户、物品的协同过滤算法1... 示例： r = Recommendor() print(&quot;items base协同推荐 slope one&quot;) #items base协同推荐算法 Slope one r.slope\\_one\\_recommendation(&#x27;lyy&#x27;) print(&quot;items base协同推荐 cos&quot;) #items base协同推荐算法 修正余弦相似度 r.cos\\_recommendation(&#x27;lyy&#x27;) print(&quot;users base协同推荐&quot;) #userbase协同推荐算法 r.user\\_base_recommendation(&quot;lyy&quot;) 复制代码 (1)基于用户的协同推荐算法这个方法是利用相似用户的喜好来进行推荐：如果要推荐一个乐队给你，会查找一个和你类似的用户，然后将他喜欢的乐队推荐给你。 算法的关键在于找到相似的用户，迭代计算你与每个用户对相同乐队的评分距离，来确定谁是你最相似的用户，距离计算可以用曼哈顿距离，皮尔斯相关系数等等。 基于用户的协同推荐算法算法的缺点： 扩展性：随着用户数量的增加，其计算量也会增加。这种算法在只有几千个用户的情况下能够工作得很好，但达到一百万个用户时就会出现瓶颈。稀疏性：大多数推荐系统中，物品的数量要远大于用户的数量，因此用户仅仅对一小部分物品进行了评价，这就造成了数据的稀疏性。比如亚马逊有上百万本书，但用户只评论了很少一部分，于是就很难找到两个相似的用户了。 (2)基于物品的协同推荐算法基于用户的协同过滤是通过计算用户之间的距离找出最相似的用户（需要将所有的评价数据在读取在内存中处理进行推荐），并将相似用户评价过的物品推荐给目标用户。而基于物品的协同过滤则是找出最相似的物品（通过构建一个物品的相似度模型来做推荐），再结合用户的评价来给出推荐结果。 基于物品的协同推荐算法常用有如下两种： 修正余弦相似度算法：以物品的评分作为物品的属性值，通过对比物品i,j的工有的用户相对评分的计算相关性s(i,j)。与皮尔逊相关系数的原理相同，共有用户对物品的每一评分R(u,j)，R(u,i)需要减去该用户评分的平均值R(`u)而消除分数膨胀。 修正余弦相似度的优点：通过构建物品模型的方式，扩展性好，占用内存小；消除分数膨胀的影响； 修正余弦相似度的缺点：稀疏性，需要基于用户的评分数据； Slope One推荐算法：第一步，计算平均差值： dev(i,j)为遍历所有共有物品i，j的共有用户u的评分平均差异。 card(Sj,i(X))则表示同时评价过物品j和i的用户数。 第二歩，使用加权的Slope One算法： PWS1(u)j表示我们将预测用户u对物品j的评分。 求合集i属于S(u)-j,用户u所含的所有物品i（除了j以外）。 dev(i,j)为遍历所有共有物品i，j的共有用户u的评分平均差异。 C(ji)也就是card(Sj,i(X))表示同时评价过物品j和i的用户数。 Slope One算法优点：算法简单；扩展性好，只需要更新共有属性的用户评价，而不需要重新载入整个数据集。 Slope One算法的缺点：稀疏性，需要基于用户的评分数据； 3、分类算法(1)基于物品特征值的KNN分类算法代码实现 鸢尾花KNN分类算法1... # KNN算法 def knn(self, oj\\_list): weight\\_dict = &#123;&quot;Iris-setosa&quot;:0.0, &quot;Iris-versicolor&quot;:0.0, &quot;Iris-virginica&quot;:0.0&#125; for atuple in oj\\_list: weight\\_dict\\[atuple\\[1\\]\\] += (1.0 / atuple\\[0\\]) rel\\_class = \\[(key, value) for key, value in weight\\_dict.items()\\] #print(sorted(rel\\_class, key=lambda x:x\\[1\\], reverse=True)) rel\\_class = sorted(rel\\_class, key=lambda x:x\\[1\\], reverse=True)\\[0\\]\\[0\\] return rel\\_class ... 复制代码 前面我们讨论的协同推荐算法需要在用户产生的各种数据上面进行分析，因此也称为社会化过滤算法，而这种算法通常有数据的稀疏性，算法可扩展性以及依赖于用户的数据的缺点，而基于物品特征值分类算法可以改善这些问题。算法分为两步： 第一步、选取特征值 算法的关键在于挑取有代表区分意义的特征及分值。以Iris花的示例，选取花萼长度， 花萼宽度，花瓣长度，花瓣宽度特征值。 第二歩、计算距离 比如计算测试集与训练集特征值之间的曼哈顿距离，得到k个最近邻后并通过加权后的结果预测分类。 KNN分类算法的缺点：无法对分类结果的置信度进行量化；是被动学习的算法，每次测试需要需要遍历所有的训练集后才能分类。 (2)贝叶斯分类算法代码实现 区分新闻类别朴素贝叶斯分类算法1... def train_data(self): #训练组的条件概率 for word in self.vocabulary: for category,value in self.prob.items(): if word not in self.prob\\[category\\]: count = 0 else : count = self.prob\\[category\\]\\[word\\] #优化条件概率公式 self.prob\\[category\\]\\[word\\] = (count + 1) / (self.total\\[category\\] + len(self.vocabulary)) ... 复制代码 贝叶斯分类算法是基于概率的分类算法。相比于KNN分类算法，它是主动学习的算法，它会根据训练集建立一个模型，并用这个模型对新样本进行分类，速度也会快很多。 贝叶斯分类算法的理论基础是基于条件概率的公式（应用于现实中P(X|Y&amp;Z)不直观得出，而P(Y|X)*P(Z|X)比较直观得出），并假设已存在的子事件(y,z…实际应用中会有多个)间是相互独立的（因此也称为朴素贝叶斯），当y，z事件假设为独立便有： 如下举例推测买牛奶和有机食品，再会买绿茶的概率： 第一步：计算先验概率及条件概率 先验概率：为单独事件发生的概率，如P(买绿茶)，P(有机食品) 条件概率（后验概率）：y事件已经发生，观察y数据集后得出x发生的概率。如P(买有机食品|买绿茶)，通过以下公式计算（nc表示y数据集下x的发生频数，n为y数据集的总数）： 上式存在一个缺陷，当一个条件概率 P(y|x)为0时，整体的预测结果P(x) * P(y|x) * P(z|x)只能为0，这样便不能更全面地预测。 修正后的条件概率：（公式摘自Tom Mitchell《机器学习》。m是一个常数，表示等效样本大小。决定常数m的方法有很多，我们这里可以使用预测结果的类别来作为m，比如投票有赞成和否决两种类别，所以m就为2。p则是相应的先验概率，比如说赞成概率是0.5，那p(赞成)就是0.5。）： 第二歩：根据贝叶斯公式做出预测 由公式计算比较y&amp;z事件发生下，不同x事件发生的概率差异，如得出P（x=喜欢），P（x=不喜欢） 的概率大小，预测为概率比较大的事件。 因为P(y)*p(z)在上式都一样，因此公式可以简化为计算概率最大项而预测分类： 贝叶斯算法的优点：能够给出分类结果的置信度；它是一种主动学习算法，速度更快。 贝叶斯算法的缺点：需要特定格式；数值型数据需要转换为类别计算概率或用高斯分布计算概率； (2)逻辑回归分类算法代码实现 区分猫的图片注：逻辑回归分类算法待后续加入网络层，更新为神经网络分类算法。 1... # cost函数，计算梯度 def propagate(w, b, X, Y): m = X.shape\\[1\\] A = sigmoid(np.dot(w.T, X) + b) cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) dw = 1 / m * np.dot(X, (A - Y).T) db = 1 / m * np.sum(A - Y) ... 复制代码 逻辑回归分类算法实现了输入特征向量X，而输出Y（范围0~1）预测X的分类。 第一步，得到关于X线性回归函数 可以通过线性回归得到WX + b，其中W是权重，b是偏差值。但不能用本式表述预测的值，因为输出Y的值需要在（0~1）区间； 第二歩，通过激活函数转换 激活函数的特点是可以将线性函数转换为非线性函数，并且有输出值有限，可微分，单调性的特点。本例使用sigmoid，使输出为预测值Y=sigmoid（WX+b）； 第三歩，构建Cost函数 训练W，b更好的预测真实的类别需要构建Cost代价函数，y^为sigmoid(WX+b)的预测分类值，y为实际分类值（0或者1）： 其中L(y^,y)称为损失函数 训练的目的就是为了让L(y^,y)足够小，也就是当y实际分类值为1时，y^要尽量偏向1。y实际分类值为0时，y^尽量小接近0。 第四步，梯度下降得到Cost函数的极小值 通过对W,b两个参数求偏导，不断迭代往下坡的的位置移动（对w，b值往极小值方向做优化，其中α为学习率控制下降的幅度），全局最优解也就是代价函数（成本函数）J (w,b)这个凸函数的极小值点。 第五步、通过训练好的W,b预测分类。 4、聚类算法(1)层次聚类代码实现 狗的种类层次聚类层次聚类将每条数据都当作是一个分类，每次迭代的时候合并距离最近的两个分类，直到剩下一个分类为止。 (2)K-means++聚类代码实现 Kmean++聚类注：Kmean算法与Kmean++区别在于初始的中心点是直接随机选取k各点。 1 ... #kmean初始化随机k个中心点 #random.seed(1) #center = \\[\\[self.data\\[i\\]\\[r\\] for i in range(1, len((self.data)))\\] #for r in random.sample(range(len(self.data)), k)\\] # Kmean ++ 初始化基于距离份量随机选k个中心点 # 1.随机选择一个点 center = \\[\\] center.append(random.choice(range(len(self.data\\[0\\])))) # 2.根据距离的概率选择其他中心点 for i in range(self.k - 1): weights = \\[self.distance_closest(self.data\\[0\\]\\[x\\], center) for x in range(len(self.data\\[0\\])) if x not in center\\] dp = \\[x for x in range(len(self.data\\[0\\])) if x not in center\\] total = sum(weights) #基于距离设定权重 weights = \\[weight/total for weight in weights\\] num = random.random() x = -1 i = 0 while i &lt; num : x += 1 i += weights\\[x\\] center.append(dp\\[x\\]) ... 复制代码 k-means++算法可概括为： （1）基于各点到中心点得距离分量，依次随机选取到k个元素作为中心点： 先随机选择一个点。重复以下步骤，直到选完k个点。 计算每个数据点dp(n)到各个中心点的距离（D），选取最小的值D(dp)； 根据D(dp)距离所占的份量来随机选取下一个点作为中心点。 （2）根据各点到中心点的距离分类； （3）计算各个分类新的中心点。 重复(2、3)，直至满足条件。","categories":[],"tags":[],"author":"Kevin"},{"title":"关于微表情的一些看法","slug":"2020-国赛初赛","date":"2021-05-16T11:38:03.000Z","updated":"2021-05-19T05:48:16.808Z","comments":true,"path":"2021/05/16/2020-国赛初赛/","link":"","permalink":"https://kevinascd.github.io/2021/05/16/2020-%E5%9B%BD%E8%B5%9B%E5%88%9D%E8%B5%9B/","excerpt":"","text":"一个2010年的实验专门调查了收看Lie to Me 的观众，对比了他们和一般人的测谎能力是否存在区别。结果表明，在所有的被试中，那些努力运用从Lie to Me学到的测谎技巧的观众，判断对方是否说谎的平均准确率达到60%，而一般人的准确率为65%。这两个数据说明，通过Lie to Me自学测谎的观众，与普通人的测谎能力并没有什么不同。（所以，大家最好还是抱着娱乐的心态，看看就好。） Aamodt 和Custer则研究了那些专门接受过微表情／肢体语言观解读训练的警察，发现他们其实与普通人一样，有着大概50%的判断准确率，他们只是比一般人更相信自己有看穿谎言的能力罢了。 为什么辨别微表情并不可靠呢？这是因为，这些技巧大多聚焦于肢体语言或面部表情的异常特征，譬如脸微微泛红，局促地笑，或者较高频率的眨眼。这些研究都存在着一个假设，即对于不同的人来说，撒谎的行为会不由自主地激发一些情绪，并表现出相似的身体反应特征。正如Ekman博士的理论提出的：“人们说谎时的特征具有普遍性。” 但实际上，心理学家们后来发现，这些看似可靠的特征其实不太管用。有的人说谎就紧张得笑出声，有的人却会脸部紧绷；有的人说谎时会与你对视，有的人却眼神闪烁。——也就是说，从来都不存在一部有关肢体语言的辞典，能够在普遍意义上定义出人们的表情代表了什么。Thomas Ormerod指出：“每个人说谎时的小动作都是不一样的。”而且，在一些特殊的情境下（譬如在警察局接受审讯，或者面临多人质询时），人们可能会表现出异常的肢体语言，只是为了减轻心理上的压力。因此，那些通过微表情来识别谎言的技巧其实太过绝对了。 那么，真正的FBI是如何测谎的？ 研究者认为，尽管我们不能通过微表情和肢体语言来看穿谎言，却可以用谈话的方式来得到我们想要的答案。伍尔弗汉普顿大学的Ormerod和Coral Dando列出了以下几种可以用来识别谎言者的谈话技巧，这些方法都已经在实际的FBI和警方的审讯过程中得到运用。 １．多问一些开放式的问题。因为撒谎者只是暗示了一个广泛的故事框架，他们的描述往往缺乏细节，因此，让他们开放地描述更容易露出破绽。不要使用是非题，一段故事中的漏洞会比单纯的“是”或“否”更容易被识别。 比如说，当你怀疑男朋友晚上去打游戏时，不要问他：“你晚上去打LOL了吗？”，而是换一种方式来问：“你晚上都干了些什么？”。当对方说自己在xx大学工作，而你表示怀疑时，你可以让对方介绍一下自己每天去上班的路途，和在大学工作的经历。 假如对方打算说谎，那么接下来他就必须得编故事。而在编织谎言的过程中，他极有可能会被自己的谎言给绕进去，也就是我们所说的“一个谎言要用千百个谎言来修补”。 ２．问一些让对方出其不意的问题，或让对方使用倒叙。英国朴次茅斯大学的社会心理学家Aldert Vrij运用了“认知负荷”理论来改良审讯方法。这一理论是说，无论人类大脑能处理的信息量是多么巨大，对我们中的大多数人来说，在某一时间点都只能进行有限的思考，在同一时间段内的认知资源（cognitive resources）是有限的，我们很难“一心二用”。 所以，如果在一般的思维过程之外增加额外的“思考”，就会产生认知负荷（cognitive load），用通俗的话来说就是“费脑子”。举个例子，当一个人一边开车一边打电话的时候，如果认知资源大部分用在了打电话上，放在开车上的认知资源不够，就容易导致事故发生。 应用在测谎上，撒谎者在应对他人的询问时，就处在“一心二用”的状态，不但要编造故事，还要考虑措辞，他们非常担心所编造故事的一贯性，所以倾向于反复使用那些考虑好了的字眼和语句。这样，我们就可以通过问一些无关的、在对方意料之外的问题，这些问题能够分散对方的认知资源，增加对方的认知负荷，从而使得对方难以维持谎言的逻辑性。譬如，跳出案件过程本身，突然询问嫌疑人在案件发生时的感官体验，或者了解案件之后所做的事。这些技巧会让对方更容易原形毕露。 关于增大认知负荷，还有一个很好用的技巧就是：让对方用倒叙的方法来叙述事件的经过。这对说谎者来说是一种陌生的叙事方式，会极大地增加认知负荷，当对方费劲地倒叙时，再去编织谎言就显得很困难，容易自相矛盾。 这一点已在实验室中得到证明。Aldert Vrij曾做过一个实验，其中说谎者和说真话者各占一半，他们被要求以倒叙的方式复述他们的说法。之后，同一批观察者观看了记录整个问询过程的录像带，并对这些说法的真实性进行评估。如果说谎者平铺直叙，谎言被辨识出的正确率只有42%——甚至在平均值之下。但如果说谎者是在倒叙，谎言被辨识的准确率会大大提高，达到60%。 ３．观察对方的自信度，适时发出挑战。当说谎者认为他们掌握了主动权的时候，他们往往会长篇大论。然而，一旦他们感觉到不对劲（或许你问到了关键的地方），他们就会变得沉默或者支支吾吾，这是因为，他们开始感觉到无法继续掌控对话的发展方向，所以不愿意多说。 所以，在谈话过程中，如果对方出现了比较明显的态度转变，我们就要考虑对方是否在说谎了。在谈话的最后，你可以尝试挑战对方的信心：“为什么我该相信你？”无论对方说了什么，再问一遍同样的问题，“你还是没有回答我，我为什么应该相信你？”接下来，再仔细聆听应答。有人可能会表示，自己所说的都是真相，而撒谎者极有可能会表现过度，气急败坏地指责你不愿意相信他们，甚至扬言用人格担保。这时，你就可以进行进一步的细节追问。 ４．关注和确认对方叙述中微小的细节。你可能还记得，当警察想知道嫌疑人与一个案子是否有关时，一般会问：“xx日xx时你在什么地方，干了什么”，然后通过查证证词的真实性，来判断嫌疑人的可靠性。如果嫌疑人在一些细节上出了错，那么他的证词就会露出破绽。 而在生活中，当你的男/女朋友说晚上在加班的时候——你就可以问：“谁在陪你一起加班？你们叫了哪家外卖？加班从几点到几点？”这些细节很容易就能通过调查来证实。 ５．“放长线钓大鱼”，不要急于拆穿。有研究者认为，撒谎者是否能够欺骗成功，依赖于他的听众是否愿意相信Ta。因此，如果你很快就发现了一处逻辑上的矛盾，也可以不要立即拆穿或纠正，而是假装自己已经相信，这样，撒谎者可能会抖露出更多的破绽，出现一个接一个的矛盾。 最难识别的撒谎者，是自己都信以为真的人 上述谈话技巧的有效性在实验和现实中都得到了充分检验。Ormerod在机场安检人员那里做了一个实验：他们准备了一些假的“旅客”，用一个星期的时间编造他们的行程，然后去到机场，与其他真的旅客一起排队接受盘查。当这些“旅客”被盘问的时候，就会说出自己编造的行程试图骗过工作人员。事实表明，使用以上谈话技巧训练的工作人员，判断对方为真假旅客的准确率，比使用肢体语言作为标准的工作人员的准确率高出20倍，能够排查出70%的“假旅客”。 而在对真正的联邦调查员的研究中，当探员们通过策略性的问题引导对方讲故事，而不去刻意关注肢体语言时，这些探员能够以平均90%的准确度成功识别出作弊者，其中有人甚至达到了100%的准确度。追踪研究表明，即使是新上手不久的探员，也能够做到80%的成功预测。 研究者认为，说到底，有效运用这些技巧的关键要求的是提问者保持开放的心态，切勿跳入预先设定的结论之中。仅仅因为有些人看起来手足无措，或是极力回想起某些细节，并不能够证明他们有罪；相反，更普遍的不一致性和矛盾之处才是我们考察的重点。 不过，有趣的是，很多老练的撒谎者更有可能识别出别人的谎言。伦敦大学学院的研究者Geoffrey Bird及其同事设计了一个实验，让被试在游戏中识别彼此是否在说谎。结果证明，那些平日里关于扯谎的人更能发现谁在吹牛，可能是因为他们更熟悉那些撒谎的把戏。 最后，最难识别的撒谎者是什么样的？是那些自己都信以为真的人。对于这样的人，上述的识别方法可能都完全不能奏效，因为他们已经活在了谎言里。","categories":[],"tags":[],"author":"Kevin"},{"title":"深度学习常用数学知识","slug":"深度学习常用数学知识","date":"2021-05-13T02:48:15.000Z","updated":"2021-06-18T10:18:27.284Z","comments":true,"path":"2021/05/13/深度学习常用数学知识/","link":"","permalink":"https://kevinascd.github.io/2021/05/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/","excerpt":"","text":"https://www.paddlepaddle.org.cn/tutorials/projectdetail/695184","categories":[],"tags":[],"author":"Kevin"},{"title":"这就是我","slug":"这就是我","date":"2021-04-28T09:01:23.000Z","updated":"2021-04-28T09:03:33.387Z","comments":true,"path":"2021/04/28/这就是我/","link":"","permalink":"https://kevinascd.github.io/2021/04/28/%E8%BF%99%E5%B0%B1%E6%98%AF%E6%88%91/","excerpt":"","text":"你好，或许现在我的水平还很浅薄，我相信我的未来","categories":[],"tags":[],"author":"Kevin"},{"title":"python常用函数合集","slug":"python常用函数合集","date":"2021-04-28T08:59:46.000Z","updated":"2021-04-28T09:03:41.063Z","comments":true,"path":"2021/04/28/python常用函数合集/","link":"","permalink":"https://kevinascd.github.io/2021/04/28/python%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E5%90%88%E9%9B%86/","excerpt":"","text":"","categories":[],"tags":[],"author":"Kevin"},{"title":"Hello World","slug":"hello-world","date":"2021-04-25T15:11:08.252Z","updated":"2021-04-25T15:11:08.252Z","comments":true,"path":"2021/04/25/hello-world/","link":"","permalink":"https://kevinascd.github.io/2021/04/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}